{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71bb3b83",
   "metadata": {},
   "source": [
    "# 03_Preprocessing_and_Dataset_Preparation\n",
    "\n",
    "This notebook prepares the scRNA-seq dataset for model training:\n",
    "- Downsample to ~20k cells\n",
    "- Train/validation/test split\n",
    "- Normalize counts (train-based)\n",
    "- Log-transform\n",
    "- Select highly variable genes (HVGs)\n",
    "- Scale data\n",
    "- Save preprocessed splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "88209c8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/d/Study/Python Scripts/scimilarity-finetune\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd1455da",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d4e9288a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-19 12:01:57 | logger_config.py:get_logger:41 | INFO | Logger initialized successfully.\n",
      "2025-10-19 12:01:57 | logger_config.py:get_logger:41 | INFO | Logger initialized successfully.\n"
     ]
    }
   ],
   "source": [
    "import scanpy as sc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from scipy import sparse as sp\n",
    "import os, json\n",
    "import joblib\n",
    "\n",
    "from app.utils import load_raw_data, split_data\n",
    "from app.logger_config import get_logger\n",
    "logger = get_logger()\n",
    "\n",
    "\n",
    "# Set random seed\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bede94e3",
   "metadata": {},
   "source": [
    "# Load Raw Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "01015780",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-19 12:02:41 | data_utils.py:load_raw_data:32 | INFO | ✅ Loaded raw data: 65479 cells x 31460 genes.\n",
      "2025-10-19 12:02:41 | 2035203210.py:<module>:2 | INFO | Original dataset shape: (65479, 31460)\n"
     ]
    }
   ],
   "source": [
    "adata = load_raw_data()\n",
    "logger.info(f\"Original dataset shape: {adata.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f733bbc7",
   "metadata": {},
   "source": [
    "# Downsample to ~20k cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0be09fe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-19 12:02:49 | 700216280.py:<module>:6 | INFO | Downsampled dataset shape: (20000, 31460)\n"
     ]
    }
   ],
   "source": [
    "n_cells_target = 20000\n",
    "if adata.n_obs > n_cells_target:\n",
    "    idx = np.random.choice(adata.n_obs, n_cells_target, replace=False)\n",
    "    adata = adata[idx, :]\n",
    "\n",
    "logger.info(f\"Downsampled dataset shape: {adata.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95ffa389",
   "metadata": {},
   "source": [
    "# Train/Validation/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a3cecb74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-19 12:03:02 | data_utils.py:split_data:166 | INFO | ✅ Data split: Train=14000, Val=3000, Test=3000\n",
      "2025-10-19 12:03:02 | 2499359727.py:<module>:3 | INFO | Train: 14000, Val: 3000, Test: 3000\n"
     ]
    }
   ],
   "source": [
    "train_data, val_data, test_data = split_data(adata, test_frac=0.3, random_state=SEED)\n",
    "\n",
    "logger.info(f\"Train: {train_data.n_obs}, Val: {val_data.n_obs}, Test: {test_data.n_obs}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e9f5da",
   "metadata": {},
   "source": [
    "# Normalize and Log-Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8638c69a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean total counts per cell (train): 2628.77\n",
      "val normalized using train-based scaling.\n",
      "test normalized using train-based scaling.\n"
     ]
    }
   ],
   "source": [
    "# Compute total counts per cell \n",
    "train_data.obs[\"n_counts\"] = np.array(train_data.X.sum(axis=1)).flatten()\n",
    "\n",
    "# Normalize training data\n",
    "sc.pp.normalize_total(train_data, target_sum=1e4)\n",
    "sc.pp.log1p(train_data)\n",
    "\n",
    "mean_train_total = np.mean(train_data.obs[\"n_counts\"])\n",
    "print(f\"Mean total counts per cell (train): {mean_train_total:.2f}\")\n",
    "\n",
    "# Apply same normalization to val/test \n",
    "for split_data, name in zip([val_data, test_data], ['val', 'test']):\n",
    "    split_data.obs[\"n_counts\"] = np.array(split_data.X.sum(axis=1)).flatten()\n",
    "    \n",
    "    # Ensure sparse CSR format\n",
    "    if not sp.isspmatrix_csr(split_data.X):\n",
    "        split_data.X = sp.csr_matrix(split_data.X)\n",
    "    \n",
    "    # Compute scaling factors (per cell)\n",
    "    scaling_factors = (1e4 / split_data.obs[\"n_counts\"].to_numpy()) * (mean_train_total / 1e4)\n",
    "    \n",
    "    # Apply normalization \n",
    "    X_scaled = split_data.X.multiply(scaling_factors[:, None])\n",
    "    split_data.X = sp.csr_matrix(X_scaled)  # ✅ ensures type compatibility\n",
    "    \n",
    "    # Log1p transform\n",
    "    split_data.X.data = np.log1p(split_data.X.data)\n",
    "    \n",
    "    print(f\"{name} normalized using train-based scaling.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44a4bdda",
   "metadata": {},
   "source": [
    "# Scale using **train mean and std**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c2a03195",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train scaled using train std.\n",
      "val scaled using train std.\n",
      "test scaled using train std.\n"
     ]
    }
   ],
   "source": [
    "# train_means and train_stds computed as before\n",
    "train_means = np.array(train_data.X.mean(axis=0)).flatten()\n",
    "X_sq_mean = np.array(train_data.X.power(2).mean(axis=0)).flatten()\n",
    "train_stds = np.sqrt(X_sq_mean - np.square(train_means))\n",
    "train_stds[train_stds == 0] = 1  # avoid divide-by-zero\n",
    "\n",
    "def sparse_scale_std_only(adata_split, std, clip_val=10):\n",
    "    \"\"\"\n",
    "    Sparse-safe standardization: divide each column by train std.\n",
    "    Does NOT subtract mean (centering) to keep sparsity.\n",
    "    \"\"\"\n",
    "    # ensure CSR format\n",
    "    if not sp.isspmatrix_csr(adata_split.X):\n",
    "        adata_split.X = sp.csr_matrix(adata_split.X)\n",
    "\n",
    "    # create a copy to avoid modifying the view\n",
    "    X = adata_split.X.copy()\n",
    "\n",
    "    # multiply each column by 1/std (broadcast across columns)\n",
    "    # sparse CSR multiply expects shape (n_rows, n_cols) and 1D array of length n_cols\n",
    "    # but multiply only works for elementwise multiplication along same shape\n",
    "    # so we convert std to a diagonal sparse matrix for safe multiplication\n",
    "    D = sp.diags(1 / std)  # shape (n_genes, n_genes)\n",
    "    X_scaled = X @ D        # sparse matrix multiplication\n",
    "    # clip\n",
    "    X_scaled.data = np.clip(X_scaled.data, -clip_val, clip_val)\n",
    "\n",
    "    adata_split.X = X_scaled\n",
    "\n",
    "for split_data, name in zip([train_data, val_data, test_data], [\"train\", \"val\", \"test\"]):\n",
    "    sparse_scale_std_only(split_data, train_stds)\n",
    "    print(f\"{name} scaled using train std.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b46f7e2",
   "metadata": {},
   "source": [
    "# Encode Cell Type Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "59af52c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize encoder\n",
    "le = LabelEncoder()\n",
    "\n",
    "# Fit on train labels only\n",
    "train_labels = train_data.obs['cell_type'].to_numpy()\n",
    "le.fit(train_labels)\n",
    "\n",
    "# Transform labels\n",
    "train_data.obs['cell_type_encoded'] = le.transform(train_labels)\n",
    "val_data.obs['cell_type_encoded'] = le.transform(val_data.obs['cell_type'].to_numpy())\n",
    "test_data.obs['cell_type_encoded'] = le.transform(test_data.obs['cell_type'].to_numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6829289b",
   "metadata": {},
   "source": [
    "# Save processed datasets and artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bd8c3e8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved train/val/test splits and preprocessing artifacts.\n"
     ]
    }
   ],
   "source": [
    "# directories\n",
    "os.makedirs(\"data/processed\", exist_ok=True)\n",
    "os.makedirs(\"artifacts\", exist_ok=True)\n",
    "\n",
    "# Save processed splits\n",
    "train_data.write_h5ad(\"data/processed/train_data.h5ad\")\n",
    "val_data.write_h5ad(\"data/processed/val_data.h5ad\")\n",
    "test_data.write_h5ad(\"data/processed/test_data.h5ad\")\n",
    "\n",
    "# Save preprocessing artifacts for inference\n",
    "np.save(\"artifacts/gene_order.npy\", adata.var_names.to_numpy())\n",
    "np.save(\"artifacts/train_means.npy\", train_means) \n",
    "np.save(\"artifacts/train_stds.npy\", train_stds)\n",
    "np.save(\"artifacts/mean_train_total.npy\", mean_train_total)\n",
    "\n",
    "# Save encoder for inference\n",
    "joblib.dump(le, \"artifacts/label_encoder.joblib\")\n",
    "\n",
    "print(\"✅ Saved train/val/test splits and preprocessing artifacts.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24bec0ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
